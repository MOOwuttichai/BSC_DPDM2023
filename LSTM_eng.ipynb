{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbRNFWW5ftnMt7e7Yo2OV8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MOOwuttichai/BSC_DPDM2023/blob/main/LSTM_eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e5pcgegjhq8",
        "outputId": "fee2c105-8cc4-43d9-e21c-23fd8474baee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new model...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 25ms/step - accuracy: 0.6200 - loss: 0.7732 - val_accuracy: 0.6307 - val_loss: 0.7099\n",
            "Epoch 2/10\n",
            "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 23ms/step - accuracy: 0.7561 - loss: 0.5640 - val_accuracy: 0.6383 - val_loss: 0.7526\n",
            "Epoch 3/10\n",
            "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 23ms/step - accuracy: 0.8755 - loss: 0.3465 - val_accuracy: 0.6080 - val_loss: 0.8960\n",
            "Epoch 4/10\n",
            "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 23ms/step - accuracy: 0.9409 - loss: 0.1951 - val_accuracy: 0.6222 - val_loss: 1.2008\n",
            "Epoch 5/10\n",
            "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 23ms/step - accuracy: 0.9724 - loss: 0.1017 - val_accuracy: 0.6155 - val_loss: 1.4682\n",
            "Epoch 6/10\n",
            "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 23ms/step - accuracy: 0.9810 - loss: 0.0657 - val_accuracy: 0.6155 - val_loss: 1.5461\n",
            "Epoch 7/10\n",
            "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9886 - loss: 0.0330 - val_accuracy: 0.6184 - val_loss: 2.1527\n",
            "Epoch 8/10\n",
            "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 26ms/step - accuracy: 0.9906 - loss: 0.0240 - val_accuracy: 0.6032 - val_loss: 2.2722\n",
            "Epoch 9/10\n",
            "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 26ms/step - accuracy: 0.9927 - loss: 0.0313 - val_accuracy: 0.6117 - val_loss: 2.3385\n",
            "Epoch 10/10\n",
            "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9928 - loss: 0.0201 - val_accuracy: 0.6004 - val_loss: 2.3702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "Accuracy: 0.6004\n",
            "Precision: 0.6023\n",
            "Recall: 0.6004\n",
            "F1 Score: 0.6009\n",
            "\n",
            "Classification Report:\n",
            "                                               precision    recall  f1-score   support\n",
            "\n",
            "ไม่มีประโยชน์/ไม่สำคัญ (useless/unimportant)       0.68      0.67      0.68       622\n",
            "            เล่าประสบการณ์ (tell experience)       0.49      0.51      0.50       373\n",
            "                            คำถาม (Question)       0.49      0.39      0.44        61\n",
            "\n",
            "                                    accuracy                           0.60      1056\n",
            "                                   macro avg       0.55      0.53      0.54      1056\n",
            "                                weighted avg       0.60      0.60      0.60      1056\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477ms/step\n",
            "ไม่มีประโยชน์/ไม่สำคัญ (useless/unimportant)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "คำถาม (Question)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "import os\n",
        "\n",
        "# ดาวน์โหลด NLTK tokenizer (ถ้ายังไม่ได้ดาวน์โหลด)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# โหลดข้อมูลจากไฟล์ CSV\n",
        "data = pd.read_csv(\"Data_model_eng.csv\")\n",
        "\n",
        "# แยกข้อความและป้ายกำกับ\n",
        "texts = data[\"comments\"].astype(str).tolist()\n",
        "labels = data[\"label\"].astype(str).tolist()\n",
        "\n",
        "# Tokenization โดยใช้ NLTK\n",
        "tokenized_texts = [word_tokenize(text) for text in texts]\n",
        "\n",
        "# แปลงคำเป็นตัวเลข\n",
        "max_words = 5000\n",
        "max_len = 20\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(tokenized_texts)\n",
        "sequences = tokenizer.texts_to_sequences(tokenized_texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# แปลง labels เป็นตัวเลข\n",
        "unique_labels = list(set(labels))\n",
        "label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "y = np.array([label_map[label] for label in labels])\n",
        "\n",
        "# แบ่งข้อมูลเป็น train/test (70/30) โดยใช้ random_state=42\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ตรวจสอบว่ามีโมเดลที่ฝึกไว้แล้วหรือไม่\n",
        "model_path = \"lstm_text_classification.h5\"\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Loading pre-trained model...\")\n",
        "    model = load_model(model_path)\n",
        "else:\n",
        "    print(\"Creating new model...\")\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
        "        LSTM(64, return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        LSTM(32),\n",
        "        Dense(len(unique_labels), activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# ฝึกโมเดลต่อจากของเดิม\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=2, validation_data=(X_test, y_test))\n",
        "\n",
        "# บันทึกโมเดลที่ฝึกใหม่\n",
        "model.save(model_path)\n",
        "\n",
        "# ทำนายผลบนชุดทดสอบ\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "# คำนวณ Accuracy, Precision, Recall, และ F1-Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "class_report = classification_report(y_test, y_pred, target_names=unique_labels)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", class_report)\n",
        "\n",
        "# ฟังก์ชันสำหรับทำนายประเภทของข้อความ\n",
        "def predict_category(text):\n",
        "    tokenized = word_tokenize(text)\n",
        "    sequence = tokenizer.texts_to_sequences([tokenized])\n",
        "    padded = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "    pred = model.predict(padded)\n",
        "    return unique_labels[np.argmax(pred)]\n",
        "\n",
        "# ทดสอบการพยากรณ์\n",
        "print(predict_category(\"I went to Japan, it was amazing!\"))  # ควรได้ 'experience'\n",
        "print(predict_category(\"Does anyone know how to make pancakes?\"))  # ควรได้ 'question'\n"
      ]
    }
  ]
}