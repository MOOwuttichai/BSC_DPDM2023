{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1A4gQ6bK0Za-PDjBGpDpnOZga09GydSTF",
      "authorship_tag": "ABX9TyPvp7wXmTuLJavYEWSWhTFv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MOOwuttichai/BSC_DPDM2023/blob/main/%E0%B8%A7%E0%B8%B4%E0%B9%80%E0%B8%84%E0%B8%A3%E0%B8%B2%E0%B8%B0%E0%B8%AB%E0%B9%8C%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "OdBJ_Zs7Vrp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.corpus import thai_stopwords\n",
        "from pythainlp.tokenize import Tokenizer\n",
        "from pythainlp.util import normalize\n",
        "from pythainlp.corpus.common import thai_words\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xKZBC1LpVkNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kok = pd.read_csv('/content/drive/MyDrive/เชี่ยงใหม่/Data_Docter_ศ.นพ.ชวลิต(หลังสกัด)_ใหม่!!.csv',encoding='utf-8')"
      ],
      "metadata": {
        "id": "f8If7GJ0c6b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kok"
      ],
      "metadata": {
        "id": "2Tg31Rs7eiZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(kok[kok['defind_cancer_with_nlp']!='ไม่สามารถระบุได้/ไม่มั่นใจว่าเป็น'])"
      ],
      "metadata": {
        "id": "ali7U0_Pdlr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=kok[kok['defind_cancer_with_nlp']!='ไม่สามารถระบุได้/ไม่มั่นใจว่าเป็น']"
      ],
      "metadata": {
        "id": "NdJB4F0ac-w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/เชี่ยงใหม่/Data_Docter_ศ.นพ.ชวลิต(หลังสกัด)_ใหม่!!.csv',encoding='utf-8')"
      ],
      "metadata": {
        "id": "LesylDvCVdzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "5Cj2RPVaZbCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdiiIcdFVbbP"
      },
      "outputs": [],
      "source": [
        "comment=data.groupby('name').sum().reset_index()\n",
        "# สร้าง set ข้อมูลภาษาไทย\n",
        "words = set(thai_words())\n",
        "words.remove(\"โรคมะเร็ง\")\n",
        "name =['กระเพาะปัสสวะ','กระเพาะปัสสาวะ','เยื่อบุโพรงมดลูก','ปากมดลูก','เม็ดเลือดขาว','กระเพาะอาหาร','กระเพราะอาหาร','ต่อมไทรอยด์','ต่อมไทยรอยด์','ท่อน้ำดี']\n",
        "for i in name:\n",
        "    words.add(i)\n",
        "def remove_stopthai(tokens):\n",
        "    final = [word.lower()\n",
        "            for word in tokens if word not in thai_stopwords()]\n",
        "    return final\n",
        "def remove_bark(tokens):\n",
        "    final = [word.lower() for word in tokens if word not in [' ','(',')','@','#','-','...','.','=','+','..','1','2','3','4','5','6','7','8','9','0']]\n",
        "    return final\n",
        "# สร้าง list เก็บตัว nlp เพิ่อนำไปวิเคราะห์โรค อาการ เเละเพศ\n",
        "list_token =[]\n",
        "count = []\n",
        "for i in range(len(comment)):#len(comment)\n",
        "    text= comment['comments'][i]\n",
        "    custom_tokenizer = Tokenizer(words)\n",
        "    Token = custom_tokenizer.word_tokenize(normalize(str(text)))\n",
        "    Token.append('end')\n",
        "    Token_final = remove_bark(remove_stopthai(Token))\n",
        "    list_token.append(Token_final)\n",
        "    count.append(len(Token_final))\n",
        "comment['จำนวนคำ'] = count\n",
        "comment['token'] = list_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_cancer=list(set(comment['defind_cancer_with_nlp']))"
      ],
      "metadata": {
        "id": "rvn0a3lIg8w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_cancer"
      ],
      "metadata": {
        "id": "Wc8ob03dXEQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "lung_cancer=comment[comment['defind_cancer_with_nlp']=='มะเร็งปอด'].reset_index()['token']\n",
        "sum_word = []\n",
        "for i in range(len(lung_cancer)):\n",
        "    sum_word+=lung_cancer[i]\n",
        "    sum_word.remove('end')\n",
        "sum_word\n",
        "fdist = FreqDist(sum_word)\n",
        "x=fdist.most_common(20)\n",
        "word_count_table = pd.DataFrame(x,columns=['word','count'])"
      ],
      "metadata": {
        "id": "fXSwxa8GOLW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#วิธีการรักษา"
      ],
      "metadata": {
        "id": "Z9XXrWsn-Ffd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Treatment = ['ผ่าตัด','คีโม','ฉายเเสง','เเร่']\n",
        "k = []\n",
        "for i in range(len(comment)):\n",
        "  L=[]\n",
        "  for j in Treatment:\n",
        "    if j in comment['token'][i]:\n",
        "      L.append(j)\n",
        "      k.append(L)\n",
        "    else:\n",
        "      k.append('ไม่สามารถระบุได้')\n",
        "comment"
      ],
      "metadata": {
        "id": "R4Bqe6C98PeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(comment)):\n",
        "  print('ผ่าตัด' in comment['token'][i])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KwiYIG1OpZdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = pd.DataFrame()\n",
        "l = []\n",
        "for i in range(len(comment)):\n",
        "  if 'แนะนำ' in comment['token'][i]:\n",
        "    l.append(comment['token'][i])\n",
        "k['word'] = l\n",
        "k"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xSc0ZfK9nHbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12 = มะเร็งกล่องเสียง\n",
        "19 = มะเร็งไทรอยด์\n",
        "31 = มะเร็งโพรงจมูก\n",
        "มะเร็งไขกระดูก\n"
      ],
      "metadata": {
        "id": "EacXTeTYmC-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(word_count_table, x='count', y='word',text_auto=True,orientation='h')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "bTzfWrR-RNeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer,util\n",
        "import numpy as np\n",
        "sentences = list(comment['comments'])\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
        "embeddings = model.encode(sentences)\n",
        "#Normalize the embeddings to unit length\n",
        "Normalize_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "clustering_model = KMeans(n_clusters=4)\n",
        "clustering_model.fit(embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "print(cluster_assignment)\n",
        "print(len(cluster_assignment))"
      ],
      "metadata": {
        "id": "nHdQ5F2bQcir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusternd_sentences= {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "  if cluster_id not in clusternd_sentences:\n",
        "    clusternd_sentences[cluster_id] = []\n",
        "\n",
        "  clusternd_sentences[cluster_id].append(sentences[sentence_id])"
      ],
      "metadata": {
        "id": "9rWe9Dw33KEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusternd_sentences"
      ],
      "metadata": {
        "id": "QBPTTrJX9w05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: ทำการ concat Data_scraper_save (6) ถึง  Data_scraper_save (17)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your files are named Data_scraper_save (6).csv, Data_scraper_save (7).csv, ... , Data_scraper_save (17).csv\n",
        "# Replace with your actual file paths if needed\n",
        "\n",
        "df_list = []\n",
        "for i in range(6, 114):\n",
        "  file_path = f'/content/Data_scraper_save ({i}).csv'  # Replace with your actual file path\n",
        "  try:\n",
        "    df = pd.read_csv(file_path, encoding='utf-8')\n",
        "    df_list.append(df)\n",
        "  except FileNotFoundError:\n",
        "    print(f\"File not found: {file_path}\")\n",
        "\n",
        "# Concatenate all DataFrames\n",
        "final_df = pd.concat(df_list, ignore_index=True)\n",
        "final_df_drop = final_df.drop_duplicates(subset=['comments'])\n",
        "# Now you have a single DataFrame containing data from all files\n",
        "print(final_df_drop)\n"
      ],
      "metadata": {
        "id": "E2LvtI9iKCB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_drop.to_csv('reddit_twothosan.csv',index=False)"
      ],
      "metadata": {
        "id": "A2YTpxsTthgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_drop.groupby('name').sum().reset_index()"
      ],
      "metadata": {
        "id": "3BrUCJ5Xu18-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: จากข้อมูล final_df_drop ช่วยสร้าง list เก็บชื่อที่ซ้ำ\n",
        "\n",
        "duplicate_names = []\n",
        "for name, group in final_df_drop.groupby('name'):\n",
        "  if len(group) > 1:\n",
        "    duplicate_names.append(name)\n"
      ],
      "metadata": {
        "id": "Ydv0IYPPwq_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_drop[final_df_drop['name']=='myersmjsc']"
      ],
      "metadata": {
        "id": "Wf2luEH4w22r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}