{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhyQSCxuE58w8cCECHK3y4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MOOwuttichai/BSC_DPDM2023/blob/main/LSTM_thai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j37dxS9yY-KD",
        "outputId": "218d8e13-75e3-412d-92d3-2dd9f672456b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-5.0.5-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from pythainlp) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->pythainlp) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->pythainlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->pythainlp) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->pythainlp) (2025.1.31)\n",
            "Downloading pythainlp-5.0.5-py3-none-any.whl (17.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pythainlp\n",
            "Successfully installed pythainlp-5.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nFiwARySW6QZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/Data_model.csv\")\n",
        "labels = []\n",
        "for sentiment in df['label']:\n",
        "  if sentiment == \"เล่าประสบการณ์ (tell experience)\":\n",
        "    labels.append(1) #เล่าประสบการณ์ (tell experience)\n",
        "  elif sentiment == \"คำถาม (Question)\":\n",
        "    labels.append(2) #ไม่มีประโยชน์/ไม่สำคัญ (useless/unimportant)\n",
        "  else:\n",
        "    labels.append(0)\n",
        "df['label_num'] = labels"
      ],
      "metadata": {
        "id": "1qaBPGKZZj3W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_E, test_df_E = train_test_split(df, test_size=0.3, random_state=42) # 70/30 split, random_state for reproducibility\n",
        "print(\"Training set size:\", len(train_df_E))\n",
        "print(\"Testing set size:\", len(test_df_E))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QutokuRBZrBz",
        "outputId": "fc43fe55-819a-49ff-e3b8-0f44b52f2c81"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 2168\n",
            "Testing set size: 930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "# โหลดข้อมูลจากไฟล์ CSV\n",
        "data = pd.read_csv(\"/content/Data_model.csv\")\n",
        "\n",
        "# แยกข้อความและป้ายกำกับ\n",
        "texts = data[\"comments\"].astype(str).tolist()\n",
        "labels = data[\"label\"].astype(str).tolist()\n",
        "\n",
        "# Tokenization โดยใช้ PyThaiNLP\n",
        "tokenized_texts = [word_tokenize(text, keep_whitespace=False) for text in texts]\n",
        "\n",
        "# แปลงคำเป็นตัวเลข\n",
        "max_words = 5000\n",
        "max_len = 20\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(tokenized_texts)\n",
        "sequences = tokenizer.texts_to_sequences(tokenized_texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# แปลง labels เป็นตัวเลข\n",
        "unique_labels = list(set(labels))\n",
        "label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "y = np.array([label_map[label] for label in labels])\n",
        "\n",
        "# สร้างโมเดล LSTM\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dense(len(unique_labels), activation='softmax')\n",
        "])\n",
        "\n",
        "# คอมไพล์โมเดล\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# ฝึกโมเดล (ต้องมีข้อมูลเพิ่มเติมสำหรับการฝึกจริง)\n",
        "model.fit(padded_sequences, y, epochs=10, batch_size=2)\n",
        "\n",
        "# ฟังก์ชันสำหรับทำนายประเภทของข้อความ\n",
        "def predict_category(text):\n",
        "    tokenized = word_tokenize(text, keep_whitespace=False)\n",
        "    sequence = tokenizer.texts_to_sequences([tokenized])\n",
        "    padded = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "    pred = model.predict(padded)\n",
        "    return unique_labels[np.argmax(pred)]\n",
        "\n",
        "# ทดสอบการพยากรณ์\n",
        "print(predict_category(\"เคยไปญี่ปุ่นมา สนุกมาก\"))  # ควรได้ 'experience'\n",
        "print(predict_category(\"มีใครรู้วิธีทำขนมครกไหม?\"))  # ควรได้ 'question'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeiRnNgyZx3P",
        "outputId": "7046cf93-76e1-423e-edc0-9bbd84f779b4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 16ms/step - accuracy: 0.9042 - loss: 0.2921\n",
            "Epoch 2/10\n",
            "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.9485 - loss: 0.1557\n",
            "Epoch 3/10\n",
            "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.9747 - loss: 0.0810\n",
            "Epoch 4/10\n",
            "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 16ms/step - accuracy: 0.9840 - loss: 0.0559\n",
            "Epoch 5/10\n",
            "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.9913 - loss: 0.0262\n",
            "Epoch 6/10\n",
            "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.9949 - loss: 0.0222\n",
            "Epoch 7/10\n",
            "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.9956 - loss: 0.0197\n",
            "Epoch 8/10\n",
            "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 17ms/step - accuracy: 0.9959 - loss: 0.0135\n",
            "Epoch 9/10\n",
            "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.9982 - loss: 0.0102\n",
            "Epoch 10/10\n",
            "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.9979 - loss: 0.0117\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435ms/step\n",
            "ไม่มีประโยชน์/ไม่สำคัญ (useless/unimportant)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "คำถาม (Question)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "# โหลดข้อมูลจากไฟล์ CSV\n",
        "data = pd.read_csv(\"Data_model.csv\")\n",
        "\n",
        "# แยกข้อความและป้ายกำกับ\n",
        "texts = data[\"comments\"].astype(str).tolist()\n",
        "labels = data[\"label\"].astype(str).tolist()\n",
        "\n",
        "# Tokenization โดยใช้ PyThaiNLP\n",
        "tokenized_texts = [word_tokenize(text, keep_whitespace=False) for text in texts]\n",
        "\n",
        "# แปลงคำเป็นตัวเลข\n",
        "max_words = 5000\n",
        "max_len = 20\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(tokenized_texts)\n",
        "sequences = tokenizer.texts_to_sequences(tokenized_texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# แปลง labels เป็นตัวเลข\n",
        "unique_labels = list(set(labels))\n",
        "label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "y = np.array([label_map[label] for label in labels])\n",
        "\n",
        "# แบ่งข้อมูลเป็น train/test (70/30) โดยใช้ random_state=42\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# สร้างโมเดล LSTM\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dense(len(unique_labels), activation='softmax')\n",
        "])\n",
        "\n",
        "# คอมไพล์โมเดล\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# ฝึกโมเดล\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=2, validation_data=(X_test, y_test))\n",
        "\n",
        "# ฟังก์ชันสำหรับทำนายประเภทของข้อความ\n",
        "def predict_category(text):\n",
        "    tokenized = word_tokenize(text, keep_whitespace=False)\n",
        "    sequence = tokenizer.texts_to_sequences([tokenized])\n",
        "    padded = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "    pred = model.predict(padded)\n",
        "    return unique_labels[np.argmax(pred)]\n",
        "\n",
        "# ทดสอบการพยากรณ์\n",
        "print(predict_category(\"เคยไปญี่ปุ่นมา สนุกมาก\"))  # ควรได้ 'experience'\n",
        "print(predict_category(\"มีใครรู้วิธีทำขนมครกไหม?\"))  # ควรได้ 'question'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY7uD9j1XPzg",
        "outputId": "c8dc99b4-3f4f-402c-f43d-e6cdb5809c01"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 20ms/step - accuracy: 0.8918 - loss: 0.3251 - val_accuracy: 0.9247 - val_loss: 0.2230\n",
            "Epoch 2/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9408 - loss: 0.1706 - val_accuracy: 0.9204 - val_loss: 0.2077\n",
            "Epoch 3/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 19ms/step - accuracy: 0.9665 - loss: 0.0969 - val_accuracy: 0.9194 - val_loss: 0.2417\n",
            "Epoch 4/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.9840 - loss: 0.0601 - val_accuracy: 0.9086 - val_loss: 0.3159\n",
            "Epoch 5/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.9896 - loss: 0.0356 - val_accuracy: 0.9043 - val_loss: 0.2998\n",
            "Epoch 6/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.9970 - loss: 0.0107 - val_accuracy: 0.8903 - val_loss: 0.4699\n",
            "Epoch 7/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0070 - val_accuracy: 0.9097 - val_loss: 0.5033\n",
            "Epoch 8/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 18ms/step - accuracy: 0.9908 - loss: 0.0308 - val_accuracy: 0.8989 - val_loss: 0.5427\n",
            "Epoch 9/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 0.9097 - val_loss: 0.4699\n",
            "Epoch 10/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 9.7170e-04 - val_accuracy: 0.9108 - val_loss: 0.5101\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452ms/step\n",
            "ไม่มีประโยชน์/ไม่สำคัญ (useless/unimportant)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "ไม่มีประโยชน์/ไม่สำคัญ (useless/unimportant)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "# โหลดข้อมูลจากไฟล์ CSV\n",
        "data = pd.read_csv(\"Data_model.csv\")\n",
        "\n",
        "# แยกข้อความและป้ายกำกับ\n",
        "texts = data[\"comments\"].astype(str).tolist()\n",
        "labels = data[\"label\"].astype(str).tolist()\n",
        "\n",
        "# Tokenization โดยใช้ PyThaiNLP\n",
        "tokenized_texts = [word_tokenize(text, keep_whitespace=False) for text in texts]\n",
        "\n",
        "# แปลงคำเป็นตัวเลข\n",
        "max_words = 5000\n",
        "max_len = 20\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(tokenized_texts)\n",
        "sequences = tokenizer.texts_to_sequences(tokenized_texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# แปลง labels เป็นตัวเลข\n",
        "unique_labels = list(set(labels))\n",
        "label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "y = np.array([label_map[label] for label in labels])\n",
        "\n",
        "# แบ่งข้อมูลเป็น train/test (70/30) โดยใช้ random_state=42\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# สร้างโมเดล LSTM\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dense(len(unique_labels), activation='softmax')\n",
        "])\n",
        "\n",
        "# คอมไพล์โมเดล\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# ฝึกโมเดล\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=2, validation_data=(X_test, y_test))\n",
        "\n",
        "# ทำนายผลบนชุดทดสอบ\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "# คำนวณ Accuracy, Precision, Recall, และ F1-Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "class_report = classification_report(y_test, y_pred, target_names=unique_labels)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", class_report)\n",
        "\n",
        "# ฟังก์ชันสำหรับทำนายประเภทของข้อความ\n",
        "def predict_category(text):\n",
        "    tokenized = word_tokenize(text, keep_whitespace=False)\n",
        "    sequence = tokenizer.texts_to_sequences([tokenized])\n",
        "    padded = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "    pred = model.predict(padded)\n",
        "    return unique_labels[np.argmax(pred)]\n",
        "\n",
        "# ทดสอบการพยากรณ์\n",
        "print(predict_category(\"เคยไปญี่ปุ่นมา สนุกมาก\"))  # ควรได้ 'experience'\n",
        "print(predict_category(\"มีใครรู้วิธีทำขนมครกไหม?\"))  # ควรได้ 'question'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLATel9ifC3W",
        "outputId": "cba58b31-9c3f-43c5-863e-ead4446acb8d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - accuracy: 0.8891 - loss: 0.3213 - val_accuracy: 0.9280 - val_loss: 0.2438\n",
            "Epoch 2/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.9553 - loss: 0.1407 - val_accuracy: 0.9280 - val_loss: 0.2144\n",
            "Epoch 3/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 19ms/step - accuracy: 0.9765 - loss: 0.0806 - val_accuracy: 0.9237 - val_loss: 0.2785\n",
            "Epoch 4/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9936 - loss: 0.0230 - val_accuracy: 0.9054 - val_loss: 0.3515\n",
            "Epoch 5/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.9965 - loss: 0.0135 - val_accuracy: 0.9247 - val_loss: 0.3149\n",
            "Epoch 6/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9956 - loss: 0.0170 - val_accuracy: 0.9140 - val_loss: 0.3723\n",
            "Epoch 7/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0109 - val_accuracy: 0.9118 - val_loss: 0.4420\n",
            "Epoch 8/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 20ms/step - accuracy: 0.9986 - loss: 0.0085 - val_accuracy: 0.9204 - val_loss: 0.4060\n",
            "Epoch 9/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 19ms/step - accuracy: 0.9999 - loss: 0.0028 - val_accuracy: 0.9172 - val_loss: 0.4109\n",
            "Epoch 10/10\n",
            "\u001b[1m1084/1084\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9986 - loss: 0.0118 - val_accuracy: 0.9161 - val_loss: 0.4617\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
            "Accuracy: 0.9161\n",
            "Precision: 0.9124\n",
            "Recall: 0.9161\n",
            "F1 Score: 0.9140\n",
            "\n",
            "Classification Report:\n",
            "                                               precision    recall  f1-score   support\n",
            "\n",
            "                            คำถาม (Question)       0.42      0.31      0.36        26\n",
            "ไม่มีประโยชน์/ไม่สำคัญ (useless/unimportant)       0.95      0.96      0.95       690\n",
            "            เล่าประสบการณ์ (tell experience)       0.86      0.86      0.86       214\n",
            "\n",
            "                                    accuracy                           0.92       930\n",
            "                                   macro avg       0.74      0.71      0.72       930\n",
            "                                weighted avg       0.91      0.92      0.91       930\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "ไม่มีประโยชน์/ไม่สำคัญ (useless/unimportant)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "ไม่มีประโยชน์/ไม่สำคัญ (useless/unimportant)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ทดสอบการพยากรณ์\n",
        "predict_category(\"เคยไปญี่ปุ่นมา สนุกมาก\")  # ควรได้ 'experience'\n",
        "# print(predict_category(\"มีใครรู้วิธีทำขนมครกไหม?\"))  # ควรได้ 'question'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "sn_m0tKxZPhy",
        "outputId": "c2cea3a9-4e6b-4d98-c74e-d857c7954158"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'no_info'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}